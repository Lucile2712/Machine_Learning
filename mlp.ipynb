{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "_In this notebook, every question will be marked by a blue border, and answers should be provided in cells in a green border. All code-related answers are preceded by a #TODO._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Students (to fill in)\n",
    "\n",
    " - Nguyen Y-Quynh (group A2)\n",
    " - Cossoul Lucile (group A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this lab is to dive into particular kind of neural network: the *Multi-Layer Perceptron* (MLP).\n",
    "\n",
    "To start, let us take the dataset from the previous lab (hydrodynamics of sailing boats) and use scikit-learn to train a MLP instead of our hand-made single perceptron.\n",
    "The code below is already complete and is meant to give you an idea of how to construct an MLP with scikit-learn. You can execute it, taking the time to understand the idea behind each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "import numpy as np\n",
    "dataset = np.genfromtxt(\"https://arbimo.github.io/tp-supervised-learning/tp1/yacht_hydrodynamics.data\", delimiter='')\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing: scale input data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,random_state=1, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(max_iter=3000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(max_iter=3000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(max_iter=3000, random_state=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a multi-layer perceptron (MLP) network for regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(max_iter=3000, random_state=1) # define the model, with default params\n",
    "mlp.fit(x_train, y_train) # train the MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain score: \u001b[39m\u001b[38;5;124m'\u001b[39m, mlp\u001b[38;5;241m.\u001b[39mscore(x_train, y_train))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest score:  \u001b[39m\u001b[38;5;124m'\u001b[39m, mlp\u001b[38;5;241m.\u001b[39mscore(x_test, y_test))\n",
      "File \u001b[1;32mc:\\Users\\cosso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py:887\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[1;32m--> 887\u001b[0m     \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[0;32m    890\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cosso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:559\u001b[0m, in \u001b[0;36m_get_data_path\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data_path\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_path\u001b[49m(), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('Train score: ', mlp.score(x_train, y_train))\n",
    "print('Test score:  ', mlp.score(x_test, y_test))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "num_samples_to_plot = 20\n",
    "plt.plot(y_test[0:num_samples_to_plot], 'ro', label='y')\n",
    "yw = mlp.predict(x_test)\n",
    "plt.plot(yw[0:num_samples_to_plot], 'bx', label='$\\hat{y}$')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.ylabel(\"f(examples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Analyzing the network\n",
    "\n",
    "Many details of the network are currently hidden as default parameters.\n",
    "\n",
    "Using the [documentation of the MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), answer the following questions.\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- What is the structure of the network?\n",
    "- What it is the algorithm used for training? Is there algorithm available that we mentioned during the courses?\n",
    "- How does the training algorithm decides to stop the training?\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mlp.n_layers_ :',  mlp.n_layers_)\n",
    "print('mlp.hidden_layer_sizes :',  mlp.hidden_layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "- What is the structure of the network?\n",
    "The network structure consist of a multi layer perceptron neural network; with an input layer, 3 hidden layers (with 100 neurons by layer) and an output layer.\n",
    "\n",
    "- What it is the algorithm used for training? Is there algorithm available that we mentioned during the courses?\n",
    "The algorithm used is the Adam solver, it refers to a stochastic gradient-based optimizer. The stochastic gradient descent was mentionned during the courses.\n",
    "\n",
    "- How does the training algorithm decides to stop the training?\n",
    "Several conditions can make the algorithm stop:\n",
    "    - The algorithm will stop after 3000 iterations\n",
    "    - The algorithm will stop if the loss of score does not improve by at least tol(1e-4) for 10 times.\n",
    "\n",
    "In this example, the algorithm stopped a 1600 iterations because of the tol\n",
    "    \n",
    "    \n",
    "    \n",
    "</div><!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Onto a more challenging dataset: house prices\n",
    "\n",
    "For the rest of this lab, we will use the (more challenging) [California Housing Prices dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean all previously defined variables for the sailing boats\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Import the required modules\"\"\"\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "print(f\"dataset type : {type(cal_housing)}\")\n",
    "print(f\"number of data : {len(cal_housing.data)}\")\n",
    "X_all = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)\n",
    "y_all = pd.DataFrame(cal_housing.target,columns=[\"target\"])\n",
    "\n",
    "X_all, y_all = shuffle(X_all, y_all, random_state=1)\n",
    "\n",
    "display(X_all.head(10)) # print the first 10 values\n",
    "display(y_all.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that each row of the dataset represents a **group of houses** (one district). The `target` variable denotes the average house value in units of 100.000 USD. Median Income is per 10.000 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "The dataset consists of 20,000 datas. We first extract the last 5,000 for test samples, which we will use later.\n",
    "\n",
    "For training and validation, we will use a subset consisting of only 2,000 datas to speed up computations.\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- Split those 2000 remaining dataset between a training set and a validation set (see usage of `train_test_split` function earlier)\n",
    "- Why did you choose this partition?\n",
    "- What is the purpose of each subset (train, validation, test) ?\n",
    "\n",
    "</div>\n",
    "<!-- Question End -->\n",
    "\n",
    "\n",
    "Please use the conventional names `X_train`, `X_val`, `y_train` and `y_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the last N samples for test (for later use)\n",
    "num_test_samples = 5000\n",
    "X_test, y_test = X_all[-num_test_samples:], y_all[-num_test_samples:]\n",
    "\n",
    "# only use the first N samples to limit training time\n",
    "num_samples = 2000\n",
    "X, y = X_all[:num_samples], y_all[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,random_state=1, test_size = 0.20)\n",
    "\n",
    "#x_train size\n",
    "print(f\"x_train size : {len(X_train)}\")\n",
    "#x_val size\n",
    "print(f\"x_val size : {len(X_val)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "- Why did you choose this partition?\n",
    "The standards for train-validation-test splits is 60-80% training data, 10-20% validation data, and 10-20% test data.\n",
    "Out of the 20 000, we use 7000 data for this part of the TP, 5000 will be for testing purpose and 2000 for training and validation.\n",
    "Out of those 2000 data, we chose to use 20% of the data for validation and 80% for training. (Those 2000 data for training and validation sould be the 15000 data available but to speed up computations, we only use 2000 data)).\n",
    "\n",
    "\n",
    "- What is the purpose of each subset (train, validation, test) ?\n",
    "To avoid overfitting a particular dataset, we split it in 3: train, validation and test.\n",
    "    - The training set is used to train and make the model\n",
    "    - The validation set is used to check our model after each training, it helps us change hyperparameters.\n",
    "    - The testing set is used after we ended on our final model, it helps us test our model on less biased data and know how accurate our model is. \n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Scaling the input data\n",
    "\n",
    "\n",
    "A step of **scaling** of the data is often useful to ensure that all input data centered on 0 and with a fixed variance.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). The function `StandardScaler` from `sklearn.preprocessing` computes the standard score of a sample as:\n",
    "\n",
    "```\n",
    "z = (x - u) / s\n",
    "```\n",
    "\n",
    "where `u` is the mean of the training samples, and `s` is the standard deviation of the training samples.\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- Using the `StandardScaler`, first fit this scaler on your training dataset (`X_train`), then use this fitted scaler to transform the training dataset, the validation dataset (`X_val`), and the test dataset (`X_test`).\n",
    "\n",
    "\n",
    "- Why is it important to fit the scaler only on the training data and not on the entire dataset or separately on each dataset?\n",
    "\n",
    "</div>\n",
    "<!-- Question End -->\n",
    "\n",
    "[Documentation of standard scaler in scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "print(\"Initial train data:\\n\", X_train)\n",
    "\n",
    "\n",
    "\n",
    "#fit the standard scaler with traain input dataset x_train\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "fitted_scaler = sc.fit(X_train)\n",
    "\n",
    "# tranform x_train, x_val and x_test\n",
    "std_X_train = fitted_scaler.transform(X_train)\n",
    "std_X_val = fitted_scaler.transform(X_val)\n",
    "std_X_test = fitted_scaler.transform(X_test)\n",
    "print(\"Standardized data x_train:\\n\", std_X_train)\n",
    "print(\"Standardized val data x_val:\\n\", std_X_val)\n",
    "print(\"Standardized test data x_test:\\n\", std_X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "- Why is it important to fit the scaler only on the training data and not on the entire dataset or separately on each dataset?\n",
    "\n",
    "We use the Scaler for maintaining the consistency of data points and suppress the differences in the scale of the features of the data.\n",
    "\n",
    "We only fit the training data because it will modify the variance and mean. Doing it on the whole dataset would give us biased estimates of our model (already using knowledge about the distribution of the test set to set the scale of the training set).\n",
    "We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "In this part, we are only interested in maximizing the **train score**, i.e., having the network memorize the training examples as well as possible. While doing this, you should (1) remain within two minutes of training time, and (2) obtain a score that is greater than 0.90.\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- Propose a parameterization of the network (number of neurons per layer, number of layers, epochs, learning rates) that will maximize the train score (without considering the test score). Ensure that you disable any form of internal validation checks such as early stopping to promote overfitting.\n",
    "\n",
    "- Is the **validation** score substantially smaller than the **train** score (indicator of overfitting) ?\n",
    "- Explain how the parameters you chose allow the learned model to overfit.\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Define a multi-layer perceptron (MLP) network for regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes = (200,200,200,), max_iter=4000, random_state=1, learning_rate_init=0.001, activation='tanh') # define the model, with default params\n",
    "mlp.fit(std_X_train, np.ravel(y_train)) # train the MLP\n",
    "print('Training score: ', mlp.score(std_X_train, np.ravel(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation score: ', mlp.score(std_X_val, np.ravel(y_val)))\n",
    "\n",
    "\n",
    "print('Test score: ', mlp.score(std_X_test, np.ravel(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "- Is the **validation** score substantially smaller than the **train** score (indicator of overfitting) ?\n",
    "The validation score is considerably smaller (training: 0.99 and validation: 0.67). We are indeed overfitting to the training data since when testing on another dataset (the validation one), the score is not great.\n",
    "\n",
    "- Explain how the parameters you chose allow the learned model to overfit.\n",
    "In this exemple, changing the learning rate did not change the score much (between 0.1 and 0.00001); the number of iteration either (between 2000 and 5000).\n",
    "The parameters that allowed us to overfit our model were the change of the activation function and the number of neurons in each layer (more than the number of layers itself).\n",
    "    - We double the size of each layers and add 2 more layers. A single layer with a lot of neurons has more redundancy, and thus is more likely to converge to a good model. Increasing the number of hidden layers much more than the sufficient number of layers will cause accuracy in the test set to decrease because of the overfit generated.\n",
    "    - We change the activation function from relu (rectified linear unit function) for the tanh (hyperbolic tan).  The activation function ReLU is linear while the tanh one is S-shaped and nonlinear, it is better to model after our specific data. Tanh is slower, but for our reduced dataset, it works great to overfit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "In this section, we are now interested in maximizing the ability of the network to predict the value of unseen examples, i.e., maximizing the **validation** score.\n",
    "You should experiment with the possible parameters of the network in order to obtain a good test score, ideally with a small learning time.\n",
    "\n",
    "Parameters to vary:\n",
    "\n",
    "- number and size of the hidden layers\n",
    "- activation function\n",
    "- stopping conditions\n",
    "- maximum number of iterations\n",
    "- initial learning rate value\n",
    "\n",
    "Results to present for the tested configurations:\n",
    "\n",
    "- Train/val score\n",
    "- training time\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "Present in a table the various parameters tested and the associated results. \n",
    "</div>\n",
    "<!-- Question End -->\n",
    "\n",
    "You can find a cell in the notebook a code snippet that will allow you to plot tables from python structure.\n",
    "Be methodical in the way your run your experiments and collect data. For each run, you should record the parameters and results into an external data structure.\n",
    "\n",
    "(Note that, while we encourage you to explore the solution space manually, there are existing methods in scikit-learn and other learning framework to automate this step as well, e.g., [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "mlp = MLPRegressor(activation='relu', hidden_layer_sizes = (300,300,), max_iter=5000, learning_rate_init=0.0001, early_stopping=False)\n",
    "mlp.fit(std_X_train, np.ravel(y_train)) # train the MLP\n",
    "print('Training score: ', mlp.score(std_X_train, np.ravel(y_train)))\n",
    "print('Validation score: ', mlp.score(std_X_val, np.ravel(y_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>nb layers</th>\n",
       "      <th>size</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.76</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.76</td>\n",
       "      <td>43.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.74</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.73</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.72</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>True</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.67</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation  nb layers  size  max_iter  learning rate  early_stopping  \\\n",
       "3        relu          3   200      3000         0.0010           False   \n",
       "11       relu          4   300      5000         0.0001           False   \n",
       "7        tanh          3   100      3000         0.0010           False   \n",
       "0        relu          3   100      3000         0.0010           False   \n",
       "6        tanh          3   200      3000         0.0010           False   \n",
       "10       relu          4   200      5000         0.0010           False   \n",
       "5        relu          3   200      3000         0.0001           False   \n",
       "8        relu          3   300      3000         0.0010           False   \n",
       "9        relu          3   300      3000         0.0010           False   \n",
       "2        relu          4   100      3000         0.0010           False   \n",
       "4        relu          3   200      3000         0.0010            True   \n",
       "1        relu          5   100      3000         0.0010           False   \n",
       "\n",
       "    train_score  val_score  time  \n",
       "3          0.85       0.76   8.0  \n",
       "11         0.90       0.76  43.3  \n",
       "7          0.77       0.75  10.3  \n",
       "0          0.83       0.74   5.6  \n",
       "6          0.78       0.74  11.3  \n",
       "10         0.91       0.74   8.8  \n",
       "5          0.75       0.73  12.2  \n",
       "8          0.84       0.73   7.7  \n",
       "9          0.84       0.73   7.7  \n",
       "2          0.83       0.72   4.1  \n",
       "4          0.73       0.71   6.3  \n",
       "1          0.97       0.67  11.6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code snippet to display a nice table in jupyter notebooks  (remove from report)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = []\n",
    "\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.83, 'val_score': 0.74, 'time' : 5.6})\n",
    "data.append({'activation': 'relu', 'nb layers' : 5, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.97, 'val_score': 0.67, 'time' : 11.6})\n",
    "data.append({'activation': 'relu', 'nb layers' : 4, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.83, 'val_score': 0.72, 'time' : 4.1})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.85, 'val_score': 0.76,  'time' : 8.0})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': True, 'train_score': 0.73, 'val_score': 0.71,  'time' : 6.3})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.0001, 'early_stopping': False, 'train_score': 0.75, 'val_score': 0.73,  'time' : 12.2})\n",
    "data.append({'activation': 'tanh', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.78, 'val_score': 0.74,  'time' : 11.3})\n",
    "data.append({'activation': 'tanh', 'nb layers' : 3, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.77, 'val_score': 0.75,  'time' : 10.3})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 300, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.84, 'val_score': 0.73,  'time' : 7.7})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 300, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.84, 'val_score': 0.73,  'time' : 7.7})\n",
    "data.append({'activation': 'relu', 'nb layers' : 4, 'size' : 200, 'max_iter': 5000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.91, 'val_score': 0.74,  'time' : 8.8})\n",
    "data.append({'activation': 'relu', 'nb layers' : 4, 'size' : 300, 'max_iter': 5000, 'learning rate' : 0.0001, 'early_stopping': False, 'train_score': 0.90, 'val_score': 0.76,  'time' : 43.3})\n",
    "\n",
    "\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "table = table.replace(np.nan, '-')\n",
    "table = table.sort_values(by='val_score', ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
    "- Evaluate the score of your model on the test set that was not used for training nor for model selection.\n",
    "- Train a model using your optimal parameters on the initial 15,000 data points. Evaluate the performance using the test set. What are your thoughts on the amount of data used? Do you believe the time spent is worthwhile in terms of the improvement in performance?\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Best model : (76% on validation)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "mlp = MLPRegressor(activation='relu', hidden_layer_sizes = (200,), max_iter=3000, learning_rate_init=0.001, early_stopping=False)\n",
    "mlp.fit(std_X_train, np.ravel(y_train)) # train the MLP\n",
    "print('Training score: ', mlp.score(std_X_train, np.ravel(y_train)))\n",
    "print('Test score: ', mlp.score(std_X_test, np.ravel(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#new test set on 15000 first data (5000 last for test)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_test_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mX_all\u001b[49m[\u001b[38;5;241m-\u001b[39mnum_test_samples:], y_all[\u001b[38;5;241m-\u001b[39mnum_test_samples:]\n\u001b[0;32m      5\u001b[0m num_train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15000\u001b[39m\n\u001b[0;32m      6\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X_all[:num_train_samples], y_all[:num_train_samples]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_all' is not defined"
     ]
    }
   ],
   "source": [
    "#new test set on 15000 first data (5000 last for test)\n",
    "num_test_samples = 5000\n",
    "X_test, y_test = X_all[-num_test_samples:], y_all[-num_test_samples:]\n",
    "\n",
    "num_train_samples = 15000\n",
    "X_train, y_train = X_all[:num_train_samples], y_all[:num_train_samples]\n",
    "\n",
    "\n",
    "#fit the standard scaler with traain input dataset x_train\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "fitted_scaler = sc.fit(X_train)\n",
    "\n",
    "# tranform x_train and x_test\n",
    "std_X_train = fitted_scaler.transform(X_train)\n",
    "std_X_test = fitted_scaler.transform(X_test)\n",
    "print(\"Standardized data x_train:\\n\", std_X_train)\n",
    "print(\"Standardized test data x_test:\\n\", std_X_test)\n",
    "\n",
    "#Best model : with 15000 data\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "mlp = MLPRegressor(activation='relu', hidden_layer_sizes = (200,), max_iter=3000, learning_rate_init=0.001, early_stopping=False)\n",
    "mlp.fit(std_X_train, np.ravel(y_train)) # train the MLP\n",
    "print('Training score: ', mlp.score(std_X_train, np.ravel(y_train)))\n",
    "print('Test score: ', mlp.score(std_X_test, np.ravel(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
    "\n",
    "From our experiments, we found that the best model used:\n",
    "    activation function: rectangle linear unit function\n",
    "    number of layers: 3\n",
    "    size of each layer: 200\n",
    "    max number of iteration: 3000\n",
    "    learning rate: 0.001\n",
    "    early stopping: no\n",
    "\n",
    "\n",
    "With these parameters, we obtained a score of 0.85 with the training set, 0.76 with the training set in a time of 8 seconds. \n",
    "    \n",
    "\n",
    "Another good contender used:\n",
    "    activation function: rectangle linear unit function\n",
    "    number of layers: 4\n",
    "    size of each layer: 300\n",
    "    max number of iteration: 5000\n",
    "    learning rate: 0.0001\n",
    "    early stopping: no\n",
    "\n",
    "\n",
    "With these parameters, we obtained a score of 0.90 with the training set, 0.76 with the training set but in a time of 43 seconds. It is too slow, considering we are using a smaller subset. With all of the data, it would take too long.\n",
    "    \n",
    "\n",
    "\n",
    "- Train a model using your optimal parameters on the initial 15,000 data points. Evaluate the performance using the test set. What are your thoughts on the amount of data used? Do you believe the time spent is worthwhile in terms of the improvement in performance?\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
