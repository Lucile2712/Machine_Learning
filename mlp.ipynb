{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "_In this notebook, every question will be marked by a blue border, and answers should be provided in cells in a green border. All code-related answers are preceded by a #TODO._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Students (to fill in)\n",
    "\n",
    " - Nguyen Y-Quynh (group A2)\n",
    " - Cossoul Lucile (group A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this lab is to dive into particular kind of neural network: the *Multi-Layer Perceptron* (MLP).\n",
    "\n",
    "To start, let us take the dataset from the previous lab (hydrodynamics of sailing boats) and use scikit-learn to train a MLP instead of our hand-made single perceptron.\n",
    "The code below is already complete and is meant to give you an idea of how to construct an MLP with scikit-learn. You can execute it, taking the time to understand the idea behind each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "import numpy as np\n",
    "dataset = np.genfromtxt(\"https://arbimo.github.io/tp-supervised-learning/tp1/yacht_hydrodynamics.data\", delimiter='')\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing: scale input data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,random_state=1, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(max_iter=3000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(max_iter=3000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(max_iter=3000, random_state=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a multi-layer perceptron (MLP) network for regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(max_iter=3000, random_state=1) # define the model, with default params\n",
    "mlp.fit(x_train, y_train) # train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9940765369322633\n",
      "Test score:   0.9899773031580283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('Train score: ', mlp.score(x_train, y_train))\n",
    "print('Test score:  ', mlp.score(x_test, y_test))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Analyzing the network\n",
    "\n",
    "Many details of the network are currently hidden as default parameters.\n",
    "\n",
    "Using the [documentation of the MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), answer the following questions.\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- What is the structure of the network?\n",
    "- What it is the algorithm used for training? Is there algorithm available that we mentioned during the courses?\n",
    "- How does the training algorithm decides to stop the training?\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.n_layers_ : 3\n",
      "mlp.hidden_layer_sizes : (100,)\n"
     ]
    }
   ],
   "source": [
    "print('mlp.n_layers_ :',  mlp.n_layers_)\n",
    "print('mlp.hidden_layer_sizes :',  mlp.hidden_layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Onto a more challenging dataset: house prices\n",
    "\n",
    "For the rest of this lab, we will use the (more challenging) [California Housing Prices dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset type : <class 'sklearn.utils._bunch.Bunch'>\n",
      "number of data : 20640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4712</th>\n",
       "      <td>3.2500</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.503205</td>\n",
       "      <td>1.073718</td>\n",
       "      <td>1109.0</td>\n",
       "      <td>1.777244</td>\n",
       "      <td>34.06</td>\n",
       "      <td>-118.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>1.9784</td>\n",
       "      <td>37.0</td>\n",
       "      <td>4.988584</td>\n",
       "      <td>1.038813</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>2.609589</td>\n",
       "      <td>36.78</td>\n",
       "      <td>-119.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15927</th>\n",
       "      <td>4.0132</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.480296</td>\n",
       "      <td>1.012315</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>3.778325</td>\n",
       "      <td>37.73</td>\n",
       "      <td>-122.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.5208</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.908046</td>\n",
       "      <td>1.114943</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.298851</td>\n",
       "      <td>37.81</td>\n",
       "      <td>-122.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>5.1795</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.406360</td>\n",
       "      <td>1.024735</td>\n",
       "      <td>711.0</td>\n",
       "      <td>2.512367</td>\n",
       "      <td>33.82</td>\n",
       "      <td>-118.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6636</th>\n",
       "      <td>7.3715</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.006098</td>\n",
       "      <td>0.993902</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1.396341</td>\n",
       "      <td>34.15</td>\n",
       "      <td>-118.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17333</th>\n",
       "      <td>5.2990</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.214932</td>\n",
       "      <td>1.047511</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2.714932</td>\n",
       "      <td>34.91</td>\n",
       "      <td>-120.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19081</th>\n",
       "      <td>2.3276</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.731076</td>\n",
       "      <td>1.115538</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>2.239044</td>\n",
       "      <td>38.31</td>\n",
       "      <td>-122.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>3.4950</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.956522</td>\n",
       "      <td>0.952569</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2.881423</td>\n",
       "      <td>34.08</td>\n",
       "      <td>-117.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7157</th>\n",
       "      <td>3.1895</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>302.0</td>\n",
       "      <td>5.033333</td>\n",
       "      <td>34.04</td>\n",
       "      <td>-118.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "4712   3.2500      39.0  4.503205   1.073718      1109.0  1.777244     34.06   \n",
       "2151   1.9784      37.0  4.988584   1.038813      1143.0  2.609589     36.78   \n",
       "15927  4.0132      46.0  4.480296   1.012315      1534.0  3.778325     37.73   \n",
       "82     1.5208      52.0  3.908046   1.114943       200.0  2.298851     37.81   \n",
       "8161   5.1795      37.0  5.406360   1.024735       711.0  2.512367     33.82   \n",
       "6636   7.3715      17.0  5.006098   0.993902       229.0  1.396341     34.15   \n",
       "17333  5.2990      12.0  7.214932   1.047511      1200.0  2.714932     34.91   \n",
       "19081  2.3276      29.0  4.731076   1.115538      1124.0  2.239044     38.31   \n",
       "13298  3.4950      35.0  4.956522   0.952569       729.0  2.881423     34.08   \n",
       "7157   3.1895      45.0  5.533333   1.166667       302.0  5.033333     34.04   \n",
       "\n",
       "       Longitude  \n",
       "4712     -118.36  \n",
       "2151     -119.78  \n",
       "15927    -122.42  \n",
       "82       -122.28  \n",
       "8161     -118.13  \n",
       "6636     -118.16  \n",
       "17333    -120.44  \n",
       "19081    -122.48  \n",
       "13298    -117.64  \n",
       "7157     -118.16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4712</th>\n",
       "      <td>3.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15927</th>\n",
       "      <td>2.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>2.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6636</th>\n",
       "      <td>2.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17333</th>\n",
       "      <td>2.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19081</th>\n",
       "      <td>1.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>1.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7157</th>\n",
       "      <td>1.563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target\n",
       "4712    3.550\n",
       "2151    0.707\n",
       "15927   2.294\n",
       "82      1.125\n",
       "8161    2.254\n",
       "6636    2.630\n",
       "17333   2.268\n",
       "19081   1.662\n",
       "13298   1.180\n",
       "7157    1.563"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Import the required modules\"\"\"\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "print(f\"dataset type : {type(cal_housing)}\")\n",
    "print(f\"number of data : {len(cal_housing.data)}\")\n",
    "X_all = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)\n",
    "y_all = pd.DataFrame(cal_housing.target,columns=[\"target\"])\n",
    "\n",
    "X_all, y_all = shuffle(X_all, y_all, random_state=1)\n",
    "\n",
    "display(X_all.head(10)) # print the first 10 values\n",
    "display(y_all.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that each row of the dataset represents a **group of houses** (one district). The `target` variable denotes the average house value in units of 100.000 USD. Median Income is per 10.000 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the last N samples for test (for later use)\n",
    "num_test_samples = 5000\n",
    "X_test, y_test = X_all[-num_test_samples:], y_all[-num_test_samples:]\n",
    "\n",
    "# only use the first N samples to limit training time\n",
    "num_samples = 2000\n",
    "X, y = X_all[:num_samples], y_all[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train size : 1600\n",
      "x_val size : 400\n"
     ]
    }
   ],
   "source": [
    "# TODO \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,random_state=1, test_size = 0.20)\n",
    "\n",
    "#x_train size\n",
    "print(f\"x_train size : {len(X_train)}\")\n",
    "#x_val size\n",
    "print(f\"x_val size : {len(X_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train data:\n",
      "        MedInc  HouseAge   AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "4712   3.2500      39.0   4.503205   1.073718      1109.0  1.777244     34.06   \n",
      "4765   2.3687      38.0   4.370968   1.008065      1019.0  2.739247     34.04   \n",
      "9998   3.4630       8.0   6.363636   1.166297      1307.0  2.898004     39.08   \n",
      "9551   2.6908      24.0   5.855204   1.004525       684.0  3.095023     37.37   \n",
      "16514  2.1186      28.0   4.707581   1.039711      1719.0  3.102888     37.80   \n",
      "...       ...       ...        ...        ...         ...       ...       ...   \n",
      "7380   2.5833      37.0   3.606164   0.900685      1354.0  4.636986     33.97   \n",
      "19680  1.8849      27.0   4.935644   1.051155      1419.0  2.341584     39.15   \n",
      "6840   2.0250      43.0   4.303279   1.032787       820.0  3.360656     34.07   \n",
      "20092  3.1076      17.0  11.428005   2.085865      2071.0  2.735799     38.03   \n",
      "19731  2.1827      26.0   4.521429   0.921429       305.0  2.178571     40.05   \n",
      "\n",
      "       Longitude  \n",
      "4712     -118.36  \n",
      "4765     -118.35  \n",
      "9998     -121.04  \n",
      "9551     -120.88  \n",
      "16514    -121.22  \n",
      "...          ...  \n",
      "7380     -118.24  \n",
      "19680    -121.63  \n",
      "6840     -118.12  \n",
      "20092    -120.19  \n",
      "19731    -122.10  \n",
      "\n",
      "[1600 rows x 8 columns]\n",
      "Standardized data x_train:\n",
      " [[-0.29979111  0.83980846 -0.52568453 ... -0.79644161 -0.75518838\n",
      "   0.62122839]\n",
      " [-0.7640933   0.75992656 -0.60485297 ... -0.15871227 -0.76440656\n",
      "   0.626241  ]\n",
      " [-0.18757465 -1.63653063  0.58812635 ... -0.05346935  1.55857634\n",
      "  -0.72215102]\n",
      " ...\n",
      " [-0.94516747  1.15933609 -0.64537734 ...  0.2532304  -0.75057928\n",
      "   0.74153102]\n",
      " [-0.37481282 -0.91759347  3.62008425 ... -0.16099808  1.07462157\n",
      "  -0.29607919]\n",
      " [-0.86208514 -0.19865632 -0.51477444 ... -0.53039426  2.00565837\n",
      "  -1.25348765]]\n",
      "Standardized val data x_val:\n",
      " [[ 1.03329364e-03  8.39808464e-01 -5.06432371e-01 ... -8.38092717e-01\n",
      "  -9.90252123e-01  9.06947144e-01]\n",
      " [-8.84739167e-01  2.80635121e-01 -1.06982809e+00 ...  1.04154986e+00\n",
      "  -8.79633889e-01  6.96417535e-01]\n",
      " [-1.56807323e-01  1.71850943e+00 -7.53569172e-01 ... -8.86056358e-01\n",
      "  -8.93461169e-01  7.31505803e-01]\n",
      " ...\n",
      " [ 2.14507987e-01 -1.15723919e+00  6.94259629e-01 ... -2.78919278e-01\n",
      "   1.36960353e+00 -8.27415825e-01]\n",
      " [ 8.56434508e-02 -1.47676681e+00 -3.86771561e-01 ... -6.72966508e-01\n",
      "   1.04235792e+00 -1.23343721e+00]\n",
      " [ 2.49015865e-01 -9.17593470e-01  6.03131573e-02 ... -4.38927964e-01\n",
      "   2.07479477e+00 -1.31865158e+00]]\n",
      "Standardized test data x_test:\n",
      " [[-0.64803093 -0.11877441 -0.955522   ... -0.63008616  0.5906668\n",
      "  -1.20837416]\n",
      " [ 0.78865587  0.36051703 -0.10202114 ... -0.48039556  0.79807599\n",
      "  -1.26351287]\n",
      " [-0.36828003 -0.43830203 -0.00884819 ... -1.06179448  0.99626699\n",
      "  -1.23844982]\n",
      " ...\n",
      " [-1.41531756  1.07945418 -0.29364658 ... -0.22115655 -0.81510659\n",
      "   0.67135449]\n",
      " [-0.62527153 -1.47676681 -0.23328168 ... -0.13594927 -0.90728845\n",
      "   1.22274156]\n",
      " [-0.79839045  0.52028084 -0.45545426 ... -0.25952073  0.96400334\n",
      "  -1.30361375]]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(\"Initial train data:\\n\", X_train)\n",
    "\n",
    "\n",
    "\n",
    "#fit the standard scaler with train input dataset x_train\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "fitted_scaler = sc.fit(X_train)\n",
    "\n",
    "# tranform x_train, x_val and x_test\n",
    "std_X_train = fitted_scaler.transform(X_train)\n",
    "std_X_val = fitted_scaler.transform(X_val)\n",
    "std_X_test = fitted_scaler.transform(X_test)\n",
    "print(\"Standardized data x_train:\\n\", std_X_train)\n",
    "print(\"Standardized val data x_val:\\n\", std_X_val)\n",
    "print(\"Standardized test data x_test:\\n\", std_X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "- Why is it important to fit the scaler only on the training data and not on the entire dataset or separately on each dataset?\n",
    "\n",
    "We use the Scaler for maintaining the consistency of data points and suppress the differences in the scale of the features of the data.\n",
    "\n",
    "We only fit the training data because it will modify the variance and mean. Doing it on the whole dataset would give us biased estimates of our model (already using knowledge about the distribution of the test set to set the scale of the training set).\n",
    "We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.993783340610986\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "# Define a multi-layer perceptron (MLP) network for regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes = (200,200,200,), max_iter=4000, random_state=1, learning_rate_init=0.001, activation='tanh') # define the model, with default params\n",
    "mlp.fit(std_X_train, np.ravel(y_train)) # train the MLP\n",
    "print('Training score: ', mlp.score(std_X_train, np.ravel(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "- Is the **validation** score substantially smaller than the **train** score (indicator of overfitting) ?\n",
    "The validation score is considerably smaller (training: 0.99 and validation: 0.67). We are indeed overfitting to the training data since when testing on another dataset (the validation one), the score is not great.\n",
    "\n",
    "- Explain how the parameters you chose allow the learned model to overfit.\n",
    "In this exemple, changing the learning rate did not change the score much (between 0.1 and 0.00001); the number of iteration either (between 2000 and 5000).\n",
    "The parameters that allowed us to overfit our model were the change of the activation function and the number of neurons in each layer (more than the number of layers itself).\n",
    "    - We double the size of each layers and add 2 more layers. A single layer with a lot of neurons has more redundancy, and thus is more likely to converge to a good model. Increasing the number of hidden layers much more than the sufficient number of layers will cause accuracy in the test set to decrease because of the overfit generated.\n",
    "    - We change the activation function from relu (rectified linear unit function) for the tanh (hyperbolic tan).  The activation function ReLU is linear while the tanh one is S-shaped and nonlinear, it is better to model after our specific data. Tanh is slower, but for our reduced dataset, it works great to overfit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "In this section, we are now interested in maximizing the ability of the network to predict the value of unseen examples, i.e., maximizing the **validation** score.\n",
    "You should experiment with the possible parameters of the network in order to obtain a good test score, ideally with a small learning time.\n",
    "\n",
    "Parameters to vary:\n",
    "\n",
    "- number and size of the hidden layers\n",
    "- activation function\n",
    "- stopping conditions\n",
    "- maximum number of iterations\n",
    "- initial learning rate value\n",
    "\n",
    "Results to present for the tested configurations:\n",
    "\n",
    "- Train/val score\n",
    "- training time\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "Present in a table the various parameters tested and the associated results. \n",
    "</div>\n",
    "<!-- Question End -->\n",
    "\n",
    "You can find a cell in the notebook a code snippet that will allow you to plot tables from python structure.\n",
    "Be methodical in the way your run your experiments and collect data. For each run, you should record the parameters and results into an external data structure.\n",
    "\n",
    "(Note that, while we encourage you to explore the solution space manually, there are existing methods in scikit-learn and other learning framework to automate this step as well, e.g., [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>nb layers</th>\n",
       "      <th>size</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.76</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.76</td>\n",
       "      <td>43.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.74</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.73</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.72</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>True</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.67</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation  nb layers  size  max_iter  learning rate  early_stopping  \\\n",
       "3        relu          3   200      3000         0.0010           False   \n",
       "11       relu          4   300      5000         0.0001           False   \n",
       "7        tanh          3   100      3000         0.0010           False   \n",
       "0        relu          3   100      3000         0.0010           False   \n",
       "6        tanh          3   200      3000         0.0010           False   \n",
       "10       relu          4   200      5000         0.0010           False   \n",
       "5        relu          3   200      3000         0.0001           False   \n",
       "8        relu          3   300      3000         0.0010           False   \n",
       "9        relu          3   300      3000         0.0010           False   \n",
       "2        relu          4   100      3000         0.0010           False   \n",
       "4        relu          3   200      3000         0.0010            True   \n",
       "1        relu          5   100      3000         0.0010           False   \n",
       "\n",
       "    train_score  val_score  time  \n",
       "3          0.85       0.76   8.0  \n",
       "11         0.90       0.76  43.3  \n",
       "7          0.77       0.75  10.3  \n",
       "0          0.83       0.74   5.6  \n",
       "6          0.78       0.74  11.3  \n",
       "10         0.91       0.74   8.8  \n",
       "5          0.75       0.73  12.2  \n",
       "8          0.84       0.73   7.7  \n",
       "9          0.84       0.73   7.7  \n",
       "2          0.83       0.72   4.1  \n",
       "4          0.73       0.71   6.3  \n",
       "1          0.97       0.67  11.6  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code snippet to display a nice table in jupyter notebooks  (remove from report)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = []\n",
    "\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.83, 'val_score': 0.74, 'time' : 5.6})\n",
    "data.append({'activation': 'relu', 'nb layers' : 5, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.97, 'val_score': 0.67, 'time' : 11.6})\n",
    "data.append({'activation': 'relu', 'nb layers' : 4, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.83, 'val_score': 0.72, 'time' : 4.1})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.85, 'val_score': 0.76,  'time' : 8.0})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': True, 'train_score': 0.73, 'val_score': 0.71,  'time' : 6.3})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.0001, 'early_stopping': False, 'train_score': 0.75, 'val_score': 0.73,  'time' : 12.2})\n",
    "data.append({'activation': 'tanh', 'nb layers' : 3, 'size' : 200, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.78, 'val_score': 0.74,  'time' : 11.3})\n",
    "data.append({'activation': 'tanh', 'nb layers' : 3, 'size' : 100, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.77, 'val_score': 0.75,  'time' : 10.3})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 300, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.84, 'val_score': 0.73,  'time' : 7.7})\n",
    "data.append({'activation': 'relu', 'nb layers' : 3, 'size' : 300, 'max_iter': 3000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.84, 'val_score': 0.73,  'time' : 7.7})\n",
    "data.append({'activation': 'relu', 'nb layers' : 4, 'size' : 200, 'max_iter': 5000, 'learning rate' : 0.001, 'early_stopping': False, 'train_score': 0.91, 'val_score': 0.74,  'time' : 8.8})\n",
    "data.append({'activation': 'relu', 'nb layers' : 4, 'size' : 300, 'max_iter': 5000, 'learning rate' : 0.0001, 'early_stopping': False, 'train_score': 0.90, 'val_score': 0.76,  'time' : 43.3})\n",
    "\n",
    "\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "table = table.replace(np.nan, '-')\n",
    "table = table.sort_values(by='val_score', ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
    "- Evaluate the score of your model on the test set that was not used for training nor for model selection.\n",
    "- Train a model using your optimal parameters on the initial 15,000 data points. Evaluate the performance using the test set. What are your thoughts on the amount of data used? Do you believe the time spent is worthwhile in terms of the improvement in performance?\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
    "\n",
    "From our experiments, we found that the best model used:\n",
    "    activation function: rectangle linear unit function\n",
    "    number of layers: 3\n",
    "    size of each layer: 200\n",
    "    max number of iteration: 3000\n",
    "    learning rate: 0.001\n",
    "    early stopping: false\n",
    "\n",
    "\n",
    "With these parameters, we obtained a score of 0.85 with the training set, 0.76 with the training set and 0.74 with the test set in a time of 8 seconds. \n",
    "    \n",
    "\n",
    "(Another good contender used:\n",
    "    activation function: rectangle linear unit function\n",
    "    number of layers: 4\n",
    "    size of each layer: 300\n",
    "    max number of iteration: 5000\n",
    "    learning rate: 0.0001\n",
    "    early stopping: no\n",
    "\n",
    "\n",
    "With these parameters, we obtained a score of 0.90 with the training set, 0.76 with the training set but in a time of 43 seconds. So way slower.)\n",
    "    \n",
    "\n",
    "\n",
    "- Train a model using your optimal parameters on the initial 15,000 data points. Evaluate the performance using the test set. What are your thoughts on the amount of data used? Do you believe the time spent is worthwhile in terms of the improvement in performance?\n",
    "\n",
    "After training this network with these parameters and 15,000 data points, it achieved an accuracy of 0.79 for the training dataset and 0.77 for the testing dataset (compared to 0.74 with the same parameters and only 1,600 datas for training).\n",
    "The amount of data has an impact on the accuracy of the model, but in our case, the improvment is not significant when adding more data (from 74% to 77%). With those parameters, the time spending is still worthwhile (from 8 seconds to 16 seconds). The time has doubled but remains very short and allows us to improve precision\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
