{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names, Surnames, and Group : (to complete by 2 students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Nguyen Y-Quynh (group A2)\n",
    " # Cossoul Lucile (group A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we consider the  (binarized) Compas dataset that we studied in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: A decision tree configuration is a set of parameters that one can use to build decision trees. Propose 6 configurations that are likely to provide different topologies and caracteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitter :  random\n",
      "max_depth : 100\n",
      "min_samples_leaf : 10\n",
      "splitter :  random\n",
      "max_depth : 1\n",
      "min_samples_leaf : 10\n",
      "splitter :  random\n",
      "max_depth : 10\n",
      "min_samples_leaf : 50\n",
      "splitter :  best\n",
      "max_depth : 20\n",
      "min_samples_leaf : 10\n",
      "splitter :  best\n",
      "max_depth : 50\n",
      "min_samples_leaf : 10\n",
      "splitter :  best\n",
      "max_depth : 100\n",
      "min_samples_leaf : 50\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt # for a good visualization of the trees \n",
    "\n",
    "train_examples, train_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "\n",
    "\n",
    "#TREE1\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=100, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE2\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=1, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE3\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=10, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE4\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=20, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE5\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=50, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "\n",
    "#TREE6\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=100, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Train a decision tree for each of the previous configurations on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_examples :  4218\n",
      "test_examples :  1055\n",
      "train_labels :  4218\n",
      "test_labels :  1055\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt # for a good visualization of the trees \n",
    "\n",
    "datas_examples, datas_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "#split 80% train and 20% test\n",
    "train_examples = datas_examples[:int(0.8*len(datas_examples))]\n",
    "test_examples = datas_examples[int(0.8*len(datas_examples)):]\n",
    "train_labels = datas_labels[:int(0.8*len(datas_labels))]\n",
    "test_labels = datas_labels[int(0.8*len(datas_labels)):]\n",
    "#print train and test\n",
    "print(\"train_examples : \", len(train_examples))\n",
    "print(\"test_examples : \", len(test_examples))\n",
    "\n",
    "\n",
    "\n",
    "#TREE1\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=100, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "\n",
    "test_results = clf.predict(test_examples)\n",
    "#plot difference between test_results and test_labels\n",
    "plt.plot(test_results, test_labels)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#TREE22\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=1, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "\n",
    "#TREE3\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=10, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "\n",
    "#TREE4\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=20, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "\n",
    "#TREE5\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=50, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "\n",
    "\n",
    "#TREE6\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=100, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Propose an evaluation in terms of training and testing accuracies using $5$-cross validation on two decision trees that have different topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Propose an experimental study that shows the transition phase from underfitting to overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Construct the confusion matrix on a particular good configuration (after explaining your choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Provide an evaluation of the fairness of the model based on the False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
