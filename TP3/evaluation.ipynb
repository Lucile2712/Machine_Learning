{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names, Surnames, and Group : (to complete by 2 students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Nguyen Y-Quynh (group A2)\n",
    " # Cossoul Lucile (group A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we consider the  (binarized) Compas dataset that we studied in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: A decision tree configuration is a set of parameters that one can use to build decision trees. Propose 6 configurations that are likely to provide different topologies and caracteristics\n",
    "Build severals decision trees (different parameters) and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitter :  random\n",
      "max_depth : 100\n",
      "min_samples_leaf : 10\n",
      "splitter :  random\n",
      "max_depth : 1\n",
      "min_samples_leaf : 10\n",
      "splitter :  random\n",
      "max_depth : 10\n",
      "min_samples_leaf : 50\n",
      "splitter :  best\n",
      "max_depth : 20\n",
      "min_samples_leaf : 10\n",
      "splitter :  best\n",
      "max_depth : 50\n",
      "min_samples_leaf : 10\n",
      "splitter :  best\n",
      "max_depth : 100\n",
      "min_samples_leaf : 50\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt # for a good visualization of the trees \n",
    "\n",
    "train_examples, train_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "\n",
    "\n",
    "#TREE1\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=100, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE2\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=1, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE3\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=10, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE4\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=20, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "#TREE5\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=50, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n",
    "\n",
    "\n",
    "#TREE6\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=100, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "#splitter\n",
    "print(\"splitter : \", clf.splitter)\n",
    "\n",
    "#max_depth\n",
    "print(\"max_depth :\", clf.max_depth)\n",
    "\n",
    "#min_samples_leaf\n",
    "print(\"min_samples_leaf :\", clf.min_samples_leaf)\n",
    "#text_representation = tree.export_text(clf, feature_names=features)\n",
    "#print(text_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "Answer :\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Train a decision tree for each of the previous configurations on the full dataset\n",
    "Run a solid evaluation on different trees (with different parameters) by randomly splitting the data 80% for training and 20% for test *multiple times*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_examples :  4218\n",
      "test_examples :  1055\n",
      "0.3582938388625592\n",
      "0.3696682464454976\n",
      "0.33270142180094786\n",
      "0.3582938388625592\n",
      "0.3582938388625592\n",
      "0.33364928909952607\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt # for a good visualization of the trees \n",
    "\n",
    "datas_examples, datas_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "#split 80% train and 20% test\n",
    "train_examples = datas_examples[:int(0.8*len(datas_examples))]\n",
    "test_examples = datas_examples[int(0.8*len(datas_examples)):]\n",
    "train_labels = datas_labels[:int(0.8*len(datas_labels))]\n",
    "test_labels = datas_labels[int(0.8*len(datas_labels)):]\n",
    "\n",
    "#print train and test\n",
    "print(\"train_examples : \", len(train_examples))\n",
    "print(\"test_examples : \", len(test_examples))\n",
    "\n",
    "\n",
    "\n",
    "#TREE1\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=100, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "test_results = clf.predict(test_examples)\n",
    "\n",
    "nb_error = np.sum(test_results != test_labels)/len(test_results)\n",
    "print(nb_error)\n",
    "\n",
    "#TREE2\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=1, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "test_results = clf.predict(test_examples)\n",
    "\n",
    "nb_error = np.sum(test_results != test_labels)/len(test_results)\n",
    "print(nb_error)\n",
    "\n",
    "#TREE3\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=10, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "test_results = clf.predict(test_examples)\n",
    "\n",
    "nb_error = np.sum(test_results != test_labels)/len(test_results)\n",
    "print(nb_error)\n",
    "\n",
    "#TREE4\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=20, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "test_results = clf.predict(test_examples)\n",
    "\n",
    "nb_error = np.sum(test_results != test_labels)/len(test_results)\n",
    "print(nb_error)\n",
    "\n",
    "#TREE5\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=50, min_samples_leaf=10)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "test_results = clf.predict(test_examples)\n",
    "\n",
    "nb_error = np.sum(test_results != test_labels)/len(test_results)\n",
    "print(nb_error)\n",
    "\n",
    "#TREE6\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=100, min_samples_leaf=50)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "test_results = clf.predict(test_examples)\n",
    "\n",
    "nb_error = np.sum(test_results != test_labels)/len(test_results)\n",
    "print(nb_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "Answer :\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Propose an evaluation in terms of training and testing accuracies using $5$-cross validation on two decision trees that have different topologies\n",
    "Do again the evaluation using 5-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores : [0.65687204 0.66635071 0.66729858 0.66603416 0.64136622]\n",
      "Mean accuracy : 0.66\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68       559\n",
      "           1       0.64      0.55      0.59       496\n",
      "\n",
      "    accuracy                           0.64      1055\n",
      "   macro avg       0.64      0.64      0.64      1055\n",
      "weighted avg       0.64      0.64      0.64      1055\n",
      "\n",
      "scores : [0.67014218 0.65308057 0.66919431 0.67267552 0.66603416]\n",
      "Mean accuracy : 0.67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       559\n",
      "           1       0.65      0.64      0.64       496\n",
      "\n",
      "    accuracy                           0.67      1055\n",
      "   macro avg       0.67      0.66      0.66      1055\n",
      "weighted avg       0.67      0.67      0.67      1055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "datas_examples, datas_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "#split 80% train and 20% test\n",
    "train_examples = datas_examples[:int(0.8*len(datas_examples))]\n",
    "test_examples = datas_examples[int(0.8*len(datas_examples)):]\n",
    "train_labels = datas_labels[:int(0.8*len(datas_labels))]\n",
    "test_labels = datas_labels[int(0.8*len(datas_labels)):]\n",
    "\n",
    "#5 cross validation\n",
    "\n",
    "\n",
    "#TREE1\n",
    "clf = tree.DecisionTreeClassifier(splitter='random', max_depth=100, min_samples_leaf=10)\n",
    "\n",
    "scores = cross_val_score(clf, datas_examples, datas_labels, cv=5)\n",
    "print('scores :', scores)\n",
    "mean_accuracy = scores.mean()\n",
    "print('Mean accuracy : %.2f' % mean_accuracy)\n",
    "\n",
    "#Train on all data\n",
    "clf.fit(train_examples, train_labels)\n",
    "\n",
    "\n",
    "#Predict labels for the test set\n",
    "prediction = clf.predict(test_examples)\n",
    "\n",
    "#Print classification report to see how our model performed (precision, recall, f-measure)\n",
    "\n",
    "print(classification_report(test_labels, prediction))\n",
    "\n",
    "#Plotting the decision tree\n",
    "#plt.figure(figsize=(15,10))\n",
    "#tree.plot_tree(clf, filled=True)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#TREE6\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', max_depth=100, min_samples_leaf=50)\n",
    "\n",
    "scores = cross_val_score(clf, datas_examples, datas_labels, cv=5)\n",
    "print('scores :', scores)\n",
    "mean_accuracy = scores.mean()\n",
    "print('Mean accuracy : %.2f' % mean_accuracy)\n",
    "\n",
    "\n",
    "#Train on all data\n",
    "clf.fit(train_examples, train_labels)\n",
    "\n",
    "\n",
    "\n",
    "#Predict labels for the test set\n",
    "prediction = clf.predict(test_examples)\n",
    "\n",
    "#Print classification report to see how our model performed (precision, recall, f-measure)\n",
    "print(classification_report(test_labels, prediction))\n",
    "\n",
    "#Plotting the decision tree\n",
    "#plt.figure(figsize=(15,10))\n",
    "#tree.plot_tree(clf, filled=True)\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "Answer :\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Propose an experimental study that shows the transition phase from underfitting to overfitting \n",
    "Evaluate the impact (in terms of accuracy) of the three parameters : maximum depth, splitting criterion, and the minimum sample leafs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores for max depth: 2  is  [0.62938389 0.63507109 0.62938389 0.64326376 0.61385199]\n",
      "Mean accuracy: 0.63\n",
      "error:  0.3696682464454976\n",
      "scores for max depth: 4  is  [0.67298578 0.65308057 0.66255924 0.67552182 0.64516129]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.3355450236966825\n",
      "scores for max depth: 6  is  [0.65687204 0.65687204 0.65592417 0.65749526 0.64895636]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.33649289099526064\n",
      "scores for max depth: 8  is  [0.6549763  0.65023697 0.65781991 0.66888046 0.64895636]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.31563981042654027\n",
      "scores for max depth: 10  is  [0.6549763  0.65971564 0.65308057 0.65844402 0.63472486]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.31563981042654027\n",
      "scores for max depth: 12  is  [0.65023697 0.6549763  0.64739336 0.65654649 0.62903226]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.31469194312796206\n",
      "scores for max depth: 14  is  [0.65023697 0.65781991 0.64834123 0.65275142 0.62998102]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.3109004739336493\n",
      "scores for max depth: 16  is  [0.65118483 0.65687204 0.64834123 0.65180266 0.62998102]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.3109004739336493\n",
      "scores for max depth: 18  is  [0.65023697 0.65687204 0.64834123 0.64800759 0.63282732]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.3109004739336493\n",
      "scores for min samples leaf: 10  is  [0.65592417 0.66635071 0.66824645 0.66603416 0.64136622]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.3127962085308057\n",
      "scores for min samples leaf: 20  is  [0.6549763  0.66161137 0.67109005 0.67362429 0.66129032]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.31563981042654027\n",
      "scores for min samples leaf: 30  is  [0.66066351 0.66445498 0.67203791 0.67457306 0.65844402]\n",
      "Mean accuracy: 0.67\n",
      "error:  0.32132701421800947\n",
      "scores for min samples leaf: 40  is  [0.66824645 0.65971564 0.66919431 0.67552182 0.66223909]\n",
      "Mean accuracy: 0.67\n",
      "error:  0.3298578199052133\n",
      "scores for min samples leaf: 50  is  [0.67014218 0.65308057 0.66919431 0.67267552 0.66698292]\n",
      "Mean accuracy: 0.67\n",
      "error:  0.3298578199052133\n",
      "scores for min samples leaf: 60  is  [0.65876777 0.65781991 0.67014218 0.66223909 0.65939279]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.3298578199052133\n",
      "scores for min samples leaf: 70  is  [0.65876777 0.64549763 0.67298578 0.67267552 0.65085389]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.3298578199052133\n",
      "scores for min samples leaf: 80  is  [0.65592417 0.65402844 0.67488152 0.67267552 0.65464896]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.33080568720379144\n",
      "scores for min samples leaf: 90  is  [0.6492891  0.65402844 0.67488152 0.67267552 0.65464896]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.3355450236966825\n",
      "scores for min samples leaf: 100  is  [0.6436019  0.65402844 0.67488152 0.67267552 0.65464896]\n",
      "Mean accuracy: 0.66\n",
      "error:  0.33933649289099527\n",
      "scores for min samples leaf: 110  is  [0.6436019  0.65023697 0.65781991 0.66413662 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33933649289099527\n",
      "scores for min samples leaf: 120  is  [0.65118483 0.65023697 0.65781991 0.66318786 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33933649289099527\n",
      "scores for min samples leaf: 130  is  [0.65118483 0.65023697 0.65781991 0.66318786 0.64421252]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33744075829383885\n",
      "scores for min samples leaf: 140  is  [0.65118483 0.65118483 0.65781991 0.66318786 0.64705882]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33744075829383885\n",
      "scores for min samples leaf: 150  is  [0.6492891  0.64834123 0.65781991 0.66318786 0.64705882]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.34407582938388626\n",
      "scores for min samples leaf: 160  is  [0.6492891  0.63981043 0.65781991 0.66318786 0.63662239]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.3459715639810427\n",
      "scores for min samples leaf: 170  is  [0.66255924 0.63412322 0.65781991 0.66318786 0.63662239]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.3459715639810427\n",
      "scores for min samples leaf: 180  is  [0.66255924 0.63412322 0.65781991 0.66318786 0.63377609]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.3459715639810427\n",
      "scores for min samples leaf: 190  is  [0.66255924 0.63601896 0.65781991 0.66318786 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.3459715639810427\n",
      "scores for min samples leaf: 200  is  [0.66255924 0.63601896 0.65781991 0.66318786 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33933649289099527\n",
      "scores for min samples leaf: 210  is  [0.66255924 0.64739336 0.65781991 0.66318786 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33933649289099527\n",
      "scores for min samples leaf: 220  is  [0.66255924 0.63507109 0.65781991 0.66318786 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33933649289099527\n",
      "scores for min samples leaf: 230  is  [0.66255924 0.63507109 0.65781991 0.66318786 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33933649289099527\n",
      "scores for min samples leaf: 240  is  [0.66255924 0.63507109 0.65781991 0.66318786 0.64326376]\n",
      "Mean accuracy: 0.65\n",
      "error:  0.33933649289099527\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_examples, train_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "\n",
    "\n",
    "#UNDERFITTING\n",
    "\n",
    "for i in range(1,10):\n",
    "    clf = tree.DecisionTreeClassifier(splitter='random', max_depth=i*2)\n",
    "\n",
    "    scores = cross_val_score(clf, datas_examples, datas_labels, cv=5)\n",
    "    print('scores for max depth:', i*2, \" is \", scores)\n",
    "    mean_accuracy = scores.mean()\n",
    "    print('Mean accuracy: %.2f' % mean_accuracy)\n",
    "    clf.fit(train_examples, train_labels)\n",
    "    prediction = clf.predict(test_examples)\n",
    "    #print the error of the prediction\n",
    "    nb_error = np.sum(prediction != test_labels)/len(prediction)\n",
    "    print('error: ', nb_error)\n",
    "\n",
    "for i in range(1,25):\n",
    "    clf = tree.DecisionTreeClassifier(splitter='random', min_samples_leaf=i*10)\n",
    "\n",
    "    scores = cross_val_score(clf, datas_examples, datas_labels, cv=5)\n",
    "    print('scores for min samples leaf:', i*10, \" is \", scores)\n",
    "    mean_accuracy = scores.mean()\n",
    "    print('Mean accuracy: %.2f' % mean_accuracy)\n",
    "    clf.fit(train_examples, train_labels)\n",
    "    prediction = clf.predict(test_examples)\n",
    "    #print the error of the prediction\n",
    "    nb_error = np.sum(prediction != test_labels)/len(prediction)\n",
    "    print('error: ', nb_error)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "Answer : \n",
    "the most underfitting one is with max_depth 2,  min_samples_leaf 1, accuracy 0.63,\n",
    "the most overfitting one is with max_depth >8, miin_samples_leaf >100, accuracy 0.65\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Construct the confusion matrix on a particular good configuration (after explaining your choice)\n",
    "Study the confusion matrix to evaluate the True/False Positive/Negative Rate. What are the most important parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[385 174]\n",
      " [186 310]]\n",
      "['race_African-American', 'race_Caucasian', 'gender_Female', 'gender_Male', 'age_18-20', 'age_21-22', 'age_23-25', 'age_26-45', 'age_>45', 'juvenile-felonies_=0', 'juvenile-felonies_>0', 'juvenile-misdemeanors_=0', 'juvenile-misdemeanors_>0', 'juvenile-crimes_=0', 'juvenile-crimes_>0', 'priors_0', 'priors_1', 'priors_2-3', 'priors_>3', 'charge_degree_Misdemeanor', 'charge_degree_Felony', 'not_gender_Female', 'not_gender_Male', 'not_age_18-20', 'not_age_21-22', 'not_age_23-25', 'not_age_26-45', 'not_age_>45', 'not_juvenile-felonies_=0', 'not_juvenile-felonies_>0', 'not_juvenile-misdemeanors_=0', 'not_juvenile-misdemeanors_>0', 'not_juvenile-crimes_=0', 'not_juvenile-crimes_>0', 'not_priors_0', 'not_priors_1', 'not_priors_2-3', 'not_priors_>3', 'not_charge_degree_Misdemeanor', 'not_charge_degree_Felony', 'juvenile-misdemeanors_=0__AND__juvenile-felonies_=0', 'gender_Male__AND__juvenile-felonies_=0', 'gender_Male__AND__juvenile-misdemeanors_=0', 'gender_Male__AND__juvenile-crimes_=0', 'juvenile-felonies_=0__AND__charge_degree_Felony', 'juvenile-misdemeanors_=0__AND__charge_degree_Felony', 'gender_Male__AND__charge_degree_Felony', 'charge_degree_Felony__AND__juvenile-crimes_=0', 'juvenile-felonies_=0__AND__priors_0', 'juvenile-misdemeanors_=0__AND__priors_0', 'gender_Male__AND__priors_0', 'charge_degree_Felony__AND__priors_0', 'juvenile-crimes_=0__AND__priors_0', 'age_26-45__AND__priors_0', 'priors_0__AND__charge_degree_Misdemeanor', 'juvenile-felonies_=0__AND__age_21-22', 'juvenile-misdemeanors_=0__AND__age_21-22', 'gender_Male__AND__age_21-22', 'charge_degree_Felony__AND__age_21-22', 'priors_0__AND__age_21-22', 'juvenile-crimes_=0__AND__age_21-22', 'age_21-22__AND__charge_degree_Misdemeanor', 'priors_2-3__AND__age_21-22', 'age_21-22__AND__gender_Female', 'priors_1__AND__age_21-22', 'gender_Male__AND__juvenile-crimes_>0', 'juvenile-felonies_=0__AND__juvenile-crimes_>0', 'juvenile-misdemeanors_=0__AND__juvenile-crimes_>0', 'charge_degree_Felony__AND__juvenile-crimes_>0', 'age_21-22__AND__juvenile-crimes_>0', 'priors_0__AND__juvenile-crimes_>0', 'age_23-25__AND__juvenile-crimes_>0', 'priors_1__AND__juvenile-crimes_>0', 'priors_2-3__AND__juvenile-crimes_>0', 'priors_>3__AND__juvenile-crimes_>0', 'juvenile-crimes_>0__AND__charge_degree_Misdemeanor', 'age_26-45__AND__juvenile-crimes_>0', 'juvenile-felonies_=0__AND__juvenile-crimes_=0', 'juvenile-misdemeanors_=0__AND__juvenile-crimes_=0', 'age_26-45__AND__juvenile-felonies_=0', 'age_26-45__AND__juvenile-crimes_=0', 'juvenile-misdemeanors_=0__AND__age_26-45', 'age_26-45__AND__charge_degree_Felony', 'gender_Male__AND__age_26-45', 'priors_>3__AND__juvenile-felonies_=0', 'priors_>3__AND__juvenile-crimes_=0', 'priors_>3__AND__juvenile-misdemeanors_=0', 'priors_>3__AND__charge_degree_Felony', 'priors_>3__AND__age_26-45', 'priors_>3__AND__gender_Male', 'priors_>3__AND__charge_degree_Misdemeanor', 'juvenile-felonies_=0__AND__gender_Female', 'juvenile-misdemeanors_=0__AND__gender_Female', 'juvenile-crimes_=0__AND__gender_Female', 'charge_degree_Felony__AND__gender_Female', 'age_26-45__AND__gender_Female', 'priors_>3__AND__gender_Female', 'priors_0__AND__gender_Female', 'charge_degree_Misdemeanor__AND__gender_Female', 'gender_Male__AND__juvenile-misdemeanors_>0', 'juvenile-felonies_=0__AND__juvenile-misdemeanors_>0', 'charge_degree_Felony__AND__juvenile-misdemeanors_>0', 'priors_>3__AND__juvenile-misdemeanors_>0', 'juvenile-crimes_=0__AND__juvenile-misdemeanors_>0', 'age_26-45__AND__juvenile-misdemeanors_>0', 'age_23-25__AND__juvenile-misdemeanors_>0', 'priors_2-3__AND__juvenile-misdemeanors_>0', 'juvenile-misdemeanors_>0__AND__charge_degree_Misdemeanor', 'juvenile-misdemeanors_>0__AND__juvenile-crimes_>0', 'juvenile-misdemeanors_>0__AND__age_21-22', 'juvenile-felonies_=0__AND__charge_degree_Misdemeanor', 'juvenile-misdemeanors_=0__AND__charge_degree_Misdemeanor', 'juvenile-crimes_=0__AND__charge_degree_Misdemeanor', 'gender_Male__AND__charge_degree_Misdemeanor', 'age_26-45__AND__charge_degree_Misdemeanor', 'priors_2-3__AND__juvenile-crimes_=0', 'priors_2-3__AND__gender_Male', 'priors_2-3__AND__charge_degree_Felony', 'priors_2-3__AND__juvenile-felonies_=0', 'priors_2-3__AND__juvenile-misdemeanors_=0', 'priors_2-3__AND__age_26-45', 'priors_2-3__AND__gender_Female', 'priors_2-3__AND__charge_degree_Misdemeanor', 'priors_2-3__AND__age_>45', 'age_23-25__AND__juvenile-crimes_=0', 'gender_Male__AND__age_23-25', 'age_23-25__AND__charge_degree_Felony', 'priors_2-3__AND__age_23-25', 'age_23-25__AND__juvenile-felonies_=0', 'juvenile-misdemeanors_=0__AND__age_23-25', 'age_23-25__AND__priors_0', 'age_23-25__AND__gender_Female', 'age_23-25__AND__priors_1', 'age_23-25__AND__charge_degree_Misdemeanor', 'priors_>3__AND__age_23-25', 'gender_Male__AND__juvenile-felonies_>0', 'charge_degree_Felony__AND__juvenile-felonies_>0', 'juvenile-crimes_=0__AND__juvenile-felonies_>0', 'priors_>3__AND__juvenile-felonies_>0', 'juvenile-misdemeanors_=0__AND__juvenile-felonies_>0', 'age_26-45__AND__juvenile-felonies_>0', 'priors_1__AND__juvenile-felonies_=0', 'juvenile-misdemeanors_=0__AND__priors_1', 'priors_1__AND__juvenile-crimes_=0', 'age_26-45__AND__priors_1', 'priors_1__AND__charge_degree_Misdemeanor', 'priors_1__AND__gender_Female', 'priors_1__AND__charge_degree_Felony', 'gender_Male__AND__priors_1', 'age_>45__AND__priors_1', 'age_>45__AND__juvenile-misdemeanors_=0', 'age_>45__AND__juvenile-crimes_=0', 'age_>45__AND__juvenile-felonies_=0', 'age_>45__AND__gender_Male', 'age_>45__AND__charge_degree_Misdemeanor', 'age_>45__AND__priors_0', 'priors_>3__AND__age_>45', 'age_>45__AND__charge_degree_Felony', 'age_>45__AND__gender_Female', 'juvenile-felonies_=0__AND__age_18-20', 'juvenile-misdemeanors_=0__AND__age_18-20', 'age_18-20__AND__charge_degree_Felony', 'gender_Male__AND__age_18-20', 'age_18-20__AND__juvenile-crimes_=0', 'age_18-20__AND__priors_0']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "datas_examples, datas_labels, features, prediction = load_from_csv(\"./compass.csv\")\n",
    "#split 80% train and 20% test\n",
    "train_examples = datas_examples[:int(0.8*len(datas_examples))]\n",
    "test_examples = datas_examples[int(0.8*len(datas_examples)):]\n",
    "train_labels = datas_labels[:int(0.8*len(datas_labels))]\n",
    "test_labels = datas_labels[int(0.8*len(datas_labels)):]\n",
    "\n",
    "\n",
    "#best model\n",
    "clf = tree.DecisionTreeClassifier(splitter='best', min_samples_leaf=30)\n",
    "clf = clf.fit(train_examples, train_labels)\n",
    "\n",
    "predictions = clf.predict(test_examples)\n",
    "conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "\n",
    "print(conf_matrix)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "Answer : We chose this configuration that was not underfitting or overfitting. It obtained the best score in the previous question with the cross_val_score function. It had the parameters\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Provide an evaluation of the fairness of the model based on the False Positive Rate.\n",
    "Propose a way to assess whether the algorithm is fair to a particular ethnic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[181 119]\n",
      " [112 230]]\n",
      "[[204  55]\n",
      " [ 74  80]]\n",
      "African American : 0.39666666666666667\n",
      "Caucasian : 0.21235521235521235\n"
     ]
    }
   ],
   "source": [
    "#evaluation of the fairness of the model (with False Positive Rate)\n",
    "#False Positive Rate = FP / (FP + TN) \n",
    "def evaluate_fairness(conf_matrix):\n",
    "    \n",
    "    TN = conf_matrix[0,0]\n",
    "    FP = conf_matrix[0,1]\n",
    "    FN = conf_matrix[1,0]\n",
    "    TP = conf_matrix[1,1]\n",
    "    FPR = FP / (FP + TN)\n",
    "    return FPR\n",
    "\n",
    "filtered_examples_african_american = []\n",
    "filtered_examples_caucasian= []\n",
    "\n",
    "filtered_labels_african_american = []\n",
    "filtered_labels_caucasian= []\n",
    "\n",
    "for i in range(len(test_examples)):\n",
    "    example = test_examples[i]\n",
    "    label = test_labels[i]\n",
    "    if example[0]==1:\n",
    "        filtered_examples_african_american.append(example)\n",
    "        filtered_labels_african_american.append(label)\n",
    "    elif example[1]==1:\n",
    "        filtered_examples_caucasian.append(example)\n",
    "        filtered_labels_caucasian.append(label)\n",
    "\n",
    "filted_predictions_african_american=clf.predict(filtered_examples_african_american)\n",
    "conf_matrix_african_american = confusion_matrix(filtered_labels_african_american, filtered_predictions_african_american)\n",
    "\n",
    "filtered_predictions_caucasian=clf.predict(filtered_examples_caucasian)\n",
    "conf_matrix_caucasian = confusion_matrix(filtered_labels_caucasian, filtered_predictions_caucasian)\n",
    "\n",
    "print(conf_matrix_african_american)\n",
    "print(conf_matrix_caucasian)\n",
    "\n",
    "#for i in range(10): \n",
    "#    print(\"african american\", filtered_predictions_african_american[i])\n",
    "#    print(\"caucasian\", filtered_predictions_caucasian[i])\n",
    "\n",
    "#evaluate fairness for population1\n",
    "print(\"African American :\", evaluate_fairness(conf_matrix_african_american))\n",
    "print(\"Caucasian :\", evaluate_fairness(conf_matrix_caucasian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "Answer :\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
